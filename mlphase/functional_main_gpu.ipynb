{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "functional_main_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOEEmIImbGT1",
        "outputId": "6cc3f21e-8ccd-4158-d28f-646acd5736bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EkAmvJtBamb",
        "outputId": "630663cb-2756-4e41-f3c6-1b1183657fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My Drive/kogaken/mlphase"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/kogaken/mlphase\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP9vdYgX-cBB"
      },
      "source": [
        "# coding: utf-8\n",
        "# import subprocess\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "# import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# from making_2D_image_20191028 import load_data\n",
        "# from making_2D_image_20191028 import surf_figure, reduce_pkl_data\n",
        "\n",
        "# from common.simple_convnet import SimpleConvNet\n",
        "# from common.trainer import Trainer\n",
        "# from dataset.mnist import load_mnist\n",
        "# import matplotlib as mpl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pij_2Q5sGnCY"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Input\n",
        "from keras.callbacks import CSVLogger, EarlyStopping\n",
        "# from keras.datasets import mnist\n",
        "# from keras.layers import Activation, Dropout, Reshape, SeparableConv2D\n",
        "# from keras.callbacks import EarlyStopping\n",
        "\n",
        "# from file_operation import MAKE_DIR, COPY_FILES\n",
        "\n",
        "# sys.path.append(os.pardir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY2evR8cAO3h"
      },
      "source": [
        "def make_fig_one_pair_abs(x, sfolder, text=\"\"):\n",
        "    temp1 = np.random.randint(0, np.shape(x[0])[0], size=4)\n",
        "\n",
        "    n = len(x)\n",
        "    print(np.shape(x[0]))\n",
        "    fig = plt.figure()\n",
        "    for i, j in enumerate(temp1):\n",
        "        for k in range(n):\n",
        "            ax = fig.add_subplot(4, n, (n * i) + k + 1)\n",
        "            # ax.set_title(\"in\")\n",
        "            ax.set_xticks([], minor=False)\n",
        "            ax.set_yticks([], minor=False)\n",
        "            ax.imshow(np.abs(x[k][j, 0, :, :]))\n",
        "\n",
        "    file = sfolder + \"/\" + \"in_out_figs_\" + text + \"_\".join(str(temp1)) + \".png\"\n",
        "    print(file)\n",
        "    plt.savefig(file)\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks2hMmkJKInn"
      },
      "source": [
        "def history_fig(history, file_train, sfolder):\n",
        "    epochs = range(len(history.history['accuracy']))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history.history['accuracy'], label='training')\n",
        "    plt.plot(epochs, history.history['val_accuracy'], label='validation')\n",
        "    plt.title('accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history.history['loss'], label='training')\n",
        "    plt.plot(epochs, history.history['val_loss'], label='validation')\n",
        "    plt.title('loss(MSE)')\n",
        "    plt.ylim([0, 0.05])\n",
        "    plt.legend()\n",
        "    plt.savefig(sfolder + \"/\" + os.path.basename(file_train).split(\".\")[0] + '.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnFcQOUSKKo8"
      },
      "source": [
        "def save_model(model, file_train, sfolder):\n",
        "    file = sfolder + \"/\" + \"model_tree.png\"\n",
        "    print(\"save_file:\", file)\n",
        "    plot_model(model, to_file=file, show_shapes=True)\n",
        "    # model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccrD2uHI_bb4"
      },
      "source": [
        "def reshape_complex_to_real_imag(x, axis=1):\n",
        "    \"\"\"RealとImag\"\"\"\n",
        "\n",
        "    return np.append(np.real(x), np.imag(x), axis=axis)\n",
        "\n",
        "\n",
        "def reshape_complex_to_abs_phase(x, axis=1):\n",
        "    \"\"\"絶対値と偏角\"\"\"\n",
        "\n",
        "    return np.append(np.abs(x), np.angle(x), axis=axis)\n",
        "\n",
        "def reshape_complex_to_real_imag_abs(x, axis=1):\n",
        "    \"\"\"RealとImagと絶対値\"\"\"\n",
        "\n",
        "    return np.append(reshape_complex_to_real_imag(x, axis=1), np.abs(x), axis=axis)\n",
        "\n",
        "def reshape_complex_to_all(x, axis=1):\n",
        "    \"\"\"絶対値と偏角とRealとImag\"\"\"\n",
        "\n",
        "    return np.append(reshape_complex_to_real_imag(x, axis=1), reshape_complex_to_abs_phase(x, axis=1), axis=axis)\n",
        "\n",
        "\n",
        "def reshape_complex(x, mode=\"abs_phase\", normalize=True):\n",
        "\n",
        "    if normalize:\n",
        "        x = x / np.max(np.abs(x))\n",
        "\n",
        "    if (\"complex\" in str(type(x[0, 0, 0, 0]))):\n",
        "        if mode == \"abs_phase\":\n",
        "            x = reshape_complex_to_abs_phase(x, axis=1)\n",
        "            if normalize:\n",
        "                x[:, 1, :, :] = (x[:, 1, :, :] + np.pi) / (2. * np.pi)\n",
        "        if mode == \"abs\":\n",
        "            x = np.abs(x)\n",
        "        if mode == \"real_imag\":\n",
        "            x = reshape_complex_to_real_imag(x, axis=1)\n",
        "        if mode == \"real_imag_abs\":\n",
        "            x = reshape_complex_to_real_imag_abs(x, axis=1)\n",
        "        if mode == \"all\":\n",
        "            x = reshape_complex_to_all(x, axis=1)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3X6sEoK_Wy2"
      },
      "source": [
        "def load_data_2(file1, file2):\n",
        "\n",
        "    new_file = os.path.dirname(file1) + \"/\" \\\n",
        "        + os.path.basename(file1).split(\".\")[0] + \"_\" \\\n",
        "        + os.path.basename(file2).split(\".\")[0] + \"_\"\n",
        "\n",
        "    if not os.path.isfile(new_file):\n",
        "        print(\"make combined file: \", new_file)\n",
        "        new_file, data_num = combine_pkl_data(file1, file2, new_file)\n",
        "\n",
        "    print(\"load_data of input\", file1, file2)\n",
        "    print(\"load_data:\", new_file)\n",
        "\n",
        "    with open(new_file, \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    z1, z2, t = [], [], []\n",
        "\n",
        "    for d in data:\n",
        "        z1.append([d[\"z1\"]])\n",
        "        z2.append([d[\"z2\"]])\n",
        "        t.append([d[\"info\"]])\n",
        "\n",
        "    z1 = np.array(z1)\n",
        "    z2 = np.array(z2)\n",
        "    t = np.array(t)\n",
        "    return z1, z2, t, data_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eSRZmcD_a13"
      },
      "source": [
        "def load_data(*files):\n",
        "    # if len(files) == 1:\n",
        "    #     return load_data_1(files[0], num_data=num_data)\n",
        "    # elif len(files) == 2:\n",
        "    #     return load_data_2(files[0], files[1])\n",
        "    if len(files) == 2:\n",
        "        return load_data_2(files[0], files[1])\n",
        "    else:\n",
        "        print(\"No Two files!!\")\n",
        "        exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq4QJR2m_JfO"
      },
      "source": [
        "def combine_pkl_data(file1, file2, new_file):\n",
        "\n",
        "    with open(file1, \"rb\") as f:\n",
        "        detec_data = pickle.load(f)\n",
        "    with open(file2, \"rb\") as f:\n",
        "        mirror_data = pickle.load(f)\n",
        "\n",
        "    i = 0\n",
        "    data = []\n",
        "    for e_data in mirror_data[\"elip\"]:\n",
        "        row = {\n",
        "            \"info\": detec_data[i][\"info\"],\n",
        "            \"z1\": detec_data[i][\"z\"],\n",
        "            \"z2\": e_data[\"z\"]\n",
        "        }\n",
        "        i += 1\n",
        "        data.append(row)\n",
        "\n",
        "    for m_data in mirror_data[\"mode\"]:\n",
        "        row = {\n",
        "            \"info\": detec_data[i][\"info\"],\n",
        "            \"z1\": detec_data[i][\"z\"],\n",
        "            \"z2\": m_data[\"z\"]\n",
        "        }\n",
        "        i += 1\n",
        "        data.append(row)\n",
        "\n",
        "    new_file += str(i) + \".pkl\"\n",
        "\n",
        "    with open(new_file, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "    return new_file, i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYLbDvl7rCtB"
      },
      "source": [
        "# def regressin_CNN(x, y, xt, yt, sfolder, active_function=\"relu\"):\n",
        "#     x = np.array(x)\n",
        "#     y = np.array(y)\n",
        "#     xt = np.array(xt)\n",
        "#     yt = np.array(yt)\n",
        "\n",
        "#     x = np.transpose(x, (0, 2, 3, 1))\n",
        "#     xt = np.transpose(xt, (0, 2, 3, 1))\n",
        "#     y = np.transpose(y, (0, 2, 3, 1))\n",
        "#     yt = np.transpose(yt, (0, 2, 3, 1))\n",
        "\n",
        "#     print(\"\")\n",
        "#     print(\"check point 3\")\n",
        "#     for s in [x, y, xt, yt]:\n",
        "#         print(np.shape(s))\n",
        "\n",
        "#     ydim = np.shape(y)\n",
        "#     out_dim = ydim[1] * ydim[2] * ydim[3]\n",
        "#     y = y.reshape(-1, out_dim)\n",
        "#     yt = yt.reshape(-1, out_dim)\n",
        "\n",
        "#     print(\"\")\n",
        "#     print(\"check point 4\")\n",
        "#     for s in [x, y, xt, yt]:\n",
        "#         print(np.shape(s))\n",
        "\n",
        "\n",
        "#     model = Sequential()\n",
        "#     model.add(Input(shape=(np.shape(x)[1], np.shape(x)[2], np.shape(x)[3])))\n",
        "#     model.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "#     # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#     model.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "    \n",
        "#     model.add(Flatten())\n",
        "\n",
        "#     model.add(Dense(8192, activation=\"relu\"))\n",
        "#     model.add(Dropout(0.25))\n",
        "#     model.add(Dense(8192, activation=\"relu\"))\n",
        "#     model.add(Dropout(0.25))\n",
        "\n",
        "#     model.add(Dense(out_dim))\n",
        "\n",
        "#     model.compile(optimizer='rmsprop',\n",
        "#                   loss='mean_squared_error',\n",
        "#                   metrics=['accuracy'])\n",
        "\n",
        "#     history = model.fit(x=x,\n",
        "#                         y=y,\n",
        "#                         epochs=1000,\n",
        "#                         validation_data=(xt, yt),\n",
        "#                         verbose=2,\n",
        "#                         callbacks=[EarlyStopping(monitor=\"val_loss\", \n",
        "#                                                  patience=20, \n",
        "#                                                  verbose=1)]\n",
        "#                         )\n",
        "#     y_pred = model.predict(xt)\n",
        "\n",
        "#     # print(history.history)\n",
        "\n",
        "#     file = \"learning\"\n",
        "#     history_fig(history, file, sfolder)\n",
        "#     # save_history(history, file, sfolder)\n",
        "#     save_model(model, file, sfolder)\n",
        "#     # save_components_fit(yt, y_pred, sfolder)\n",
        "#     # save_components_fit_9(yt, y_pred, sfolder)\n",
        "#     epochs = len(history.history['accuracy'])\n",
        "\n",
        "#     score = model.evaluate(x=xt, y=yt)\n",
        "#     rmse_score = np.sqrt(np.mean((yt - y_pred) ** 2))\n",
        "#     mae_score = np.mean(np.sqrt((yt - y_pred) ** 2))\n",
        "#     print(\"test_MSE: \", score[0])\n",
        "#     print(\"test_RMSE: \", rmse_score)\n",
        "#     print(\"test_MAE: \", mae_score)\n",
        "#     print(\"test_ACC: \", score[1])\n",
        "\n",
        "#     return rmse_score, mae_score, score, y_pred, epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUgHStbJ_4xL"
      },
      "source": [
        "def regressin_CNN(x, y, xt, yt, sfolder, active_function=\"relu\"):\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    xt = np.array(xt)\n",
        "    yt = np.array(yt)\n",
        "\n",
        "    x = np.transpose(x, (0, 2, 3, 1))\n",
        "    xt = np.transpose(xt, (0, 2, 3, 1))\n",
        "    y = np.transpose(y, (0, 2, 3, 1))\n",
        "    yt = np.transpose(yt, (0, 2, 3, 1))\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"check point 3\")\n",
        "    for s in [x, y, xt, yt]:\n",
        "        print(np.shape(s))\n",
        "\n",
        "    ydim = np.shape(y)\n",
        "    out_dim = ydim[1] * ydim[2] * ydim[3]\n",
        "    y = y.reshape(-1, out_dim)\n",
        "    yt = yt.reshape(-1, out_dim)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"check point 4\")\n",
        "    for s in [x, y, xt, yt]:\n",
        "        print(np.shape(s))\n",
        "\n",
        "\n",
        "    main_input = Input(shape=(np.shape(x)[1], np.shape(x)[2], np.shape(x)[3], ))\n",
        "    x = Conv2D(48, (3, 3), activation=\"relu\", padding=\"same\")(main_input)\n",
        "    x = Conv2D(48, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(12288, activation=\"relu\")(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(12288, activation=\"relu\")(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    predictions = Dense(out_dim)(x)\n",
        "\n",
        "    model = Model(inputs=main_input, outputs=predictions)\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    print(model.summary())\n",
        "\n",
        "    history = model.fit(x=x,\n",
        "                        y=y,\n",
        "                        epochs=1000,\n",
        "                        validation_data=(xt, yt),\n",
        "                        verbose=2,\n",
        "                        callbacks=[EarlyStopping(monitor=\"val_loss\", \n",
        "                                                 patience=20, \n",
        "                                                 verbose=1)]\n",
        "                        )\n",
        "    y_pred = model.predict(xt)\n",
        "\n",
        "    # print(history.history)\n",
        "\n",
        "    file = \"learning\"\n",
        "    history_fig(history, file, sfolder)\n",
        "    # save_history(history, file, sfolder)\n",
        "    save_model(model, file, sfolder)\n",
        "    # save_components_fit(yt, y_pred, sfolder)\n",
        "    # save_components_fit_9(yt, y_pred, sfolder)\n",
        "    epochs = len(history.history['accuracy'])\n",
        "\n",
        "    score = model.evaluate(x=xt, y=yt)\n",
        "    rmse_score = np.sqrt(np.mean((yt - y_pred) ** 2))\n",
        "    mae_score = np.mean(np.sqrt((yt - y_pred) ** 2))\n",
        "    print(\"test_MSE: \", score[0])\n",
        "    print(\"test_RMSE: \", rmse_score)\n",
        "    print(\"test_MAE: \", mae_score)\n",
        "    print(\"test_ACC: \", score[1])\n",
        "\n",
        "    return rmse_score, mae_score, score, y_pred, epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJcn6zFk-0iT"
      },
      "source": [
        "def main_regression_CNN_learn(detec_train, mirror_train, detec_test, mirror_test, \n",
        "                              sfolder, active_function, reshape_type):\n",
        "    x, y, _, train_num = load_data(detec_train, mirror_train)\n",
        "    xt, yt, _, test_num = load_data(detec_test, mirror_test)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"check point 1\")\n",
        "    for s in [x, y, xt, yt]:\n",
        "        print(np.shape(s))\n",
        "\n",
        "    x = reshape_complex(x, mode=reshape_type, normalize=True)\n",
        "    y = reshape_complex(y, mode=reshape_type, normalize=True)\n",
        "    xt = reshape_complex(xt, mode=reshape_type, normalize=True)\n",
        "    yt = reshape_complex(yt, mode=reshape_type, normalize=True)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"check point 2\")\n",
        "    for s in [x, y, xt, yt]:\n",
        "        print(np.shape(s))\n",
        "\n",
        "    rmse_score, mae_score, score, y_pred, epochs = regressin_CNN(x, y, xt, yt, sfolder,\n",
        "                                                                active_function)\n",
        "\n",
        "    y_pred_2d = y_pred.reshape(-1, 1, np.shape(yt)[2], np.shape(yt)[3])\n",
        "\n",
        "    for i in range(10):\n",
        "        make_fig_one_pair_abs((xt, yt, y_pred_2d), sfolder, \"pred\")\n",
        "    \n",
        "    file = sfolder + \"/output.json\"\n",
        "    with open(file, \"w\") as f:\n",
        "        memo = {\n",
        "                \"folder\": str(sfolder),\n",
        "                \"train_data_num\": str(train_num),\n",
        "                \"test_data_num\": str(test_num),\n",
        "                \"epochs\": str(epochs),\n",
        "                \"test_MSE\": str(score[0]),\n",
        "                \"test_RMSE\": str(rmse_score),\n",
        "                \"test_MAE\": str(mae_score),\n",
        "                \"test_ACC\": str(score[1]),\n",
        "                \"active_func\": str(active_function),\n",
        "                \"reshape_type\": str(reshape_type)\n",
        "                }\n",
        "        json.dump(memo, f, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zxc8uxXJWw-"
      },
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    session_conf = tf.compat.v1.ConfigProto(\n",
        "        intra_op_parallelism_threads=1,\n",
        "        inter_op_parallelism_threads=1\n",
        "    )\n",
        "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "    tf.compat.v1.keras.backend.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbpcgbGD-Sml"
      },
      "source": [
        "def main(data_folder, active_function, reshape_type):\n",
        "    strtime = datetime.now().strftime(\"%Y%m%d%H%M\" + \"-\" + \"%S\")\n",
        "    print(\"\")\n",
        "    print(\"time=\", strtime)\n",
        "\n",
        "    # 保存フォルダの指定,時刻がフォルダの名前になる\n",
        "    if not data_folder:\n",
        "        os.makedirs(strtime)\n",
        "        sfolder = strtime\n",
        "    else:\n",
        "        sfolder = data_folder + \"/\" + strtime\n",
        "        os.makedirs(sfolder)\n",
        "\n",
        "    mirror_train = data_folder + \"/\" + \"mirror_train_data.pkl\"\n",
        "    detec_train = data_folder + \"/\" + \"detec_train_data.pkl\"\n",
        "\n",
        "    mirror_test = data_folder + \"/\" + \"mirror_test_data.pkl\"\n",
        "    detec_test = data_folder + \"/\" + \"detec_test_data.pkl\"\n",
        "\n",
        "    # main_make_figs(file_train_x,file_train_y,sfolder,num_data=num_data)\n",
        "\n",
        "    # main_learn(file_train_x,file_test,sfolder,num_data)\n",
        "    \n",
        "    # main_regression_learn(file_train_x,file_train_y,\n",
        "    #                       file_test_x, file_test_y,\n",
        "    #                       sfolder,num_data=num_data)\n",
        "\n",
        "    seed_everything(43)\n",
        "\n",
        "    main_regression_CNN_learn(detec_train, mirror_train,\n",
        "                              detec_test, mirror_test, sfolder,\n",
        "                              active_function, reshape_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5_PRT0n-lPV",
        "outputId": "a12008f1-f928-4bf6-db42-601aaf8f3b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "folder = \"result/2020_1016_115030_add_noise_paramator_010_test1000\"\n",
        "active_function=\"relu\"\n",
        "\n",
        "# \"abs_phase\", \"abs\", \"real_imag\", \"real_imag_abs\", \"all\"\n",
        "reshape_type=\"real_imag_abs\"\n",
        "\n",
        "main(folder, active_function, reshape_type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "time= 202010231523-47\n",
            "make combined file:  result/2020_1016_115030_add_noise_paramator_010_test1000/detec_train_data_mirror_train_data_\n",
            "load_data of input result/2020_1016_115030_add_noise_paramator_010_test1000/detec_train_data.pkl result/2020_1016_115030_add_noise_paramator_010_test1000/mirror_train_data.pkl\n",
            "load_data: result/2020_1016_115030_add_noise_paramator_010_test1000/detec_train_data_mirror_train_data_5000.pkl\n",
            "make combined file:  result/2020_1016_115030_add_noise_paramator_010_test1000/detec_test_data_mirror_test_data_\n",
            "load_data of input result/2020_1016_115030_add_noise_paramator_010_test1000/detec_test_data.pkl result/2020_1016_115030_add_noise_paramator_010_test1000/mirror_test_data.pkl\n",
            "load_data: result/2020_1016_115030_add_noise_paramator_010_test1000/detec_test_data_mirror_test_data_1000.pkl\n",
            "\n",
            "check point 1\n",
            "(5000, 1, 16, 16)\n",
            "(5000, 1, 64, 64)\n",
            "(1000, 1, 16, 16)\n",
            "(1000, 1, 64, 64)\n",
            "\n",
            "check point 2\n",
            "(5000, 3, 16, 16)\n",
            "(5000, 1, 64, 64)\n",
            "(1000, 3, 16, 16)\n",
            "(1000, 1, 64, 64)\n",
            "\n",
            "check point 3\n",
            "(5000, 16, 16, 3)\n",
            "(5000, 64, 64, 1)\n",
            "(1000, 16, 16, 3)\n",
            "(1000, 64, 64, 1)\n",
            "\n",
            "check point 4\n",
            "(5000, 16, 16, 3)\n",
            "(5000, 4096)\n",
            "(1000, 16, 16, 3)\n",
            "(1000, 4096)\n",
            "Model: \"functional_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 16, 16, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 16, 16, 48)        1344      \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 16, 16, 48)        20784     \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 12288)             0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 12288)             151007232 \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 12288)             0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 12288)             151007232 \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 12288)             0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 4096)              50335744  \n",
            "=================================================================\n",
            "Total params: 352,372,336\n",
            "Trainable params: 352,372,336\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-59b30b527df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreshape_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"real_imag_abs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-65-836647339db9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_folder, active_function, reshape_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m     main_regression_CNN_learn(detec_train, mirror_train,\n\u001b[1;32m     31\u001b[0m                               \u001b[0mdetec_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmirror_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                               active_function, reshape_type)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-4f6da68ed902>\u001b[0m in \u001b[0;36mmain_regression_CNN_learn\u001b[0;34m(detec_train, mirror_train, detec_test, mirror_test, sfolder, active_function, reshape_type)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     rmse_score, mae_score, score, y_pred, epochs = regressin_CNN(x, y, xt, yt, sfolder,\n\u001b[0;32m---> 22\u001b[0;31m                                                                 active_function)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0my_pred_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-4cec3c248101>\u001b[0m in \u001b[0;36mregressin_CNN\u001b[0;34m(x, y, xt, yt, sfolder, active_function)\u001b[0m\n\u001b[1;32m     51\u001b[0m                         callbacks=[EarlyStopping(monitor=\"val_loss\", \n\u001b[1;32m     52\u001b[0m                                                  \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                                                  verbose=1)]\n\u001b[0m\u001b[1;32m     54\u001b[0m                         )\n\u001b[1;32m     55\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Data cardinality is ambiguous:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Data cardinality is ambiguous:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5tHvuvXCcrp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}